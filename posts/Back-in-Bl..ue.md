---
title: "Back in Bl..ue"
date: "2018-06-22"
---

<div class="content">
<h1 id="back-in-bl-ue">Back in Bl..ue</h1>
<p>Thanks to a gracious gift of gadgetry for Fathers Day, I was able to spend last weekend increasing Mark II’s computing power to <em>maximum capacity!</em> <img alt="IMG_0844" src="/wp/2018/06/img_0844.jpg"/> Not only did I have the parts, but I also had the time (again thanks to Jamie &amp; Berty) and had very ambitious plans for pushing Mark II across the finish line. Things didn’t go quite as planned (I spent one of the two days fighting O/S problems of all things) but I was able to get all 8 nodes online by the end of the weekend. <img alt="IMG_0848" src="/wp/2018/06/img_0848.jpg"/> Since then I’ve been working on running <a href="http://www.netlib.org/benchmark/hpl/" target="_blank">high-performance Linpack</a> (hpl) to see how Mark II’s performance stacks-up against Mark I. After a few runs, I was not only able <em>match</em> the best results I achieved with Mark I, but slightly <em>exceed</em> them. The best hpl performance I could get out of Mark I was 9.403 Gflops. Using a similar hpl configuration (adjusted for reduced memory size per node), I was able to get 9.525 out of Mark II. On both machines this result was achieved with a 4x4 configuration (4 nodes, 16 cores) so even though it wasn’t the maximum capacity of either machine, it’s a pretty good apples-to-apples comparison between the two architectures. That said, there’s more performance to be had. I’ve learned a lot about running hpl since I recored Mark I’s results and I’m confident that I can figure out what was stopping me from improving the numbers by adding nodes back then. In addition to better hpl tuning skills, I’m able to run all nodes of Mark II without throwing a circuit breaker, which was not the case with Mark I. This makes iterating on hpl configs a lot easier. The theoretical maximum hpl performance (Rpeak) for Mark II is 76 Gflops. Achieving this is impossible due to memory constraints, transport overhead, etc. but I think 75% of Rpeak is not unreasonable, which would yield a measured peak performance (Rmax) of 57 Gflops. This would rank Mark II right around the middle of the <a href="https://www.top500.org/" target="_blank">Top 500 Supercomputer Sites</a>… <a href="https://www.top500.org/list/1999/06/?page=2" target="_blank">in 1999</a>. [caption id=“attachment_5206” align=“aligncenter” width=“1005”]<img alt="crayt3e_005" src="/wp/2018/06/crayt3e_005.jpg"/> RAIN Mark II’s hpl performance should be similar to the room-sized, 300KW Cray T3E (example pictured above)[/caption] Not too shabby for a $500.00 machine you can hold in your hand. <img alt="ganglia_for_blog" src="/wp/2018/06/ganglia_for_blog.png"/> Still, there’s a long ways to go from my current results of not-quite 10 Gflops to 57. I think theres a lot to improve in how I’m configuring hpl and I also think there is work to do at the O/S and hardware level. Minimally I’m going to need to increase the machine’s cooling capacity (right now I don’t even have heatsinks on the SOC’s so I <em>know</em> the CPU throttle is kicking in). So if I’m able to find an hpl config that doesn’t <em>loose</em> performance as I add nodes to the test, and I can keep the system cool enough to make sure cpu throttling doesn’t kick-in, I should be able to get much closer to that Rmax value. But even with no improvement this test confirms the validity of the Mark II design. The original goal for this phase of the project was to establish the difference in performance between the Mark I machine and a similar cluster built from ARM single-board computers. The difference, surprisingly is that the ARM machine has a <em>slight</em> node-for-node edge over the “traditional” Mark I. Perhaps just as important, Mark II achieves this with significantly less cost, physical size, thermal output and electricity consumption. Once I complete the power supply electronics for Mark II I’ll be able to get more precise measurements of power consumption but even if Mark II operates at it’s maximum power consumption that’s still <sup>1</sup>⁄<sub>10</sub> of the power consumed by Mark I. This is a significant milestone in the RAIN project and marks the “official” end of the Mark II phase. I plan to finish Mark II’s implementation (wiring front-panel controls for all nodes, designing &amp; fabricating the front-end interface board, etc.) and generate a “reference design” for others who would like to recreate my results (perhaps even offer a kit?), but with these results in hand I can confidently enter into the Mark III phase of the project as well.</p>
</div>
